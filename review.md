# Technology Rewiew

## Tensorflow tools for text retrieval problems

### Abstract

Tensorflow is one of the advanced and most-used Machine Learning tool. Tensorflow is mostly focused on Deep Learning, but its functions and even pre-trained models can be quite helpful for traditional NLP. In this article I will describe Tensorflow features for NLP and text retrieval along with brief code examples on Python. I will cover text processing tools; ML tools for Natural language processing; existing datasets and pre-trained models; and, finally, sulitions for text retrieval problems.

### Text processing

Text processing is the first step in solving machine learning problems. In many cases programmers and researchers deal with raw data, which needs to be cleaned, processed and transformed in order to provide further NLP steps the best data possible.
While it can be done with traditional algorithms like Porter or Lancaster Stemmer, Tensorflow provides its own implementation. Here is the list of some available tokenizers: **WhitespaceTokenizer**, the most basic string tokenizer which splits string into individual words. **WordpieceTokenizer** is more complex and works as stemmer. The tokenizer uses dictionary, built with top-down WordPiece generation algorithm. There are four steps in building such dictionary: 1. Count word frequency (w, c) for the provided document. 2. For each given word generate a set of all possible substrings: *word -> ["w", "wo", "wor", "word", "#ord", "##rd", "###d"]*. 3. Build a hashmap *(S, TC)* for substring, where *TCk = sum(C0, C1, ..., Cn)*, where *Ci* is a cound of word *Wi*, which generated substring *k*. 4. Remove substrings *Si* which have *TCk* less than given threshold *T*. 5. For each of the substring left, subtract off its count from all of its prefixes. **BertTokenizer** combines the previous two, but also performs additional tasks such as normalization. **SentencepieceTokenizer** is a sub-token tokenizer that is highly configurable. This is backed by the Sentencepiece library. Like the BertTokenizer, it can include normalization and token splitting before splitting into sub-tokens. Tensorflow provides ready-to-use vocabularies, but allows you to build your own. Out-of-vocabulary word is a common issue for tokenization, so pre-trained tokenizers are usually perform better.


### Natural Language Processing tools

Here I cover two major functions used in text retrieval: text classification and ranking. Tensorflow has comprehensive, highly customizible tools for both problems.   In Tensorflow, ranking is a part of TensorFlow Recommenders (TFRS) module. It helps with the full workflow of building a recommender system: data preparation, model formulation, training, evaluation, and deployment. Module has a variety of pointwise, pairwise, and listwise loss functions. It provides **Mean Reciprocal Rank**, **Discounted Cumulative Gain**, **Normalized Discounted Cumulative Gain (NDCG)**, **Mean Average Precision** and other popular metrics. It also has **LambdaLoss**, a probabilistic framework for ranking metric optimization. Such metric-driven loss functions significantly improve ranking algorithms. Another component is **Unbiased Learning-to-Rank** from biased feedback data, which implements ranking algorithm itself. In May 2021 Google released a major update of TFRS. It uncluded **ModelBuilder**, **DatasetBuilder**, and a **Pipeline**, tools for set up and train the model with the provided dataset. The key feature of the update was new TFR-BERT architecture, a combination of learning-to-rank (LTR) models and Bidirectional Encoder Representations from Transformers (BERT) representation. Google also introduced neural ranking generalized additive models (GAM) for ranking problems requiring interpretability.

